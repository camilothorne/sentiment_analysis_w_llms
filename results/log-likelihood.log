INFO:__main__:Started
INFO:__main__:Using Apple Silicon / MPS GPU
INFO:__main__:1.5B checkpoint: Qwen/Qwen2.5-1.5B-Instruct
INFO:__main__:
================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
Qwen2ForCausalLM                                        --
├─Qwen2Model: 1-1                                       --
│    └─Embedding: 2-1                                   233,373,696
│    └─ModuleList: 2-2                                  --
│    │    └─Qwen2DecoderLayer: 3-1                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-2                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-3                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-4                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-5                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-6                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-7                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-8                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-9                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-10                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-11                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-12                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-13                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-14                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-15                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-16                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-17                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-18                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-19                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-20                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-21                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-22                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-23                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-24                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-25                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-26                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-27                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-28                     46,797,824
│    └─Qwen2RMSNorm: 2-3                                1,536
│    └─Qwen2RotaryEmbedding: 2-4                        --
├─Linear: 1-2                                           233,373,696
================================================================================
Total params: 1,777,088,000
Trainable params: 1,777,088,000
Non-trainable params: 0
================================================================================
INFO:__main__:500M model: Qwen/Qwen2.5-0.5B-Instruct
INFO:__main__:
================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
Qwen2ForCausalLM                                        --
├─Qwen2Model: 1-1                                       --
│    └─Embedding: 2-1                                   136,134,656
│    └─ModuleList: 2-2                                  --
│    │    └─Qwen2DecoderLayer: 3-1                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-2                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-3                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-4                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-5                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-6                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-7                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-8                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-9                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-10                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-11                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-12                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-13                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-14                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-15                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-16                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-17                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-18                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-19                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-20                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-21                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-22                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-23                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-24                     14,912,384
│    └─Qwen2RMSNorm: 2-3                                896
│    └─Qwen2RotaryEmbedding: 2-4                        --
├─Linear: 1-2                                           136,134,656
================================================================================
Total params: 630,167,424
Trainable params: 630,167,424
Non-trainable params: 0
================================================================================
INFO:__main__:The 1.5B model has a vocabulary of 151643 tokens.
INFO:__main__:The 500M model has a vocabulary of 151643 tokens.
INFO:__main__:Deriving a balanced random sample of 5%/500 reviews of IMDB the test set for evaluation
INFO:__main__:Downloading and reading IMDB dataset...
INFO:__main__:Test set statistics:
INFO:sentiment_analysis:Number of sentences:                  500
INFO:sentiment_analysis:Average number of words per sentence: 237.312
INFO:sentiment_analysis:Max number of words per sentence:     1057
INFO:sentiment_analysis:Min number of words per sentence:     15
INFO:sentiment_analysis:Total number of words:                118656
INFO:__main__:Time taken for 1.5B model: 829.4854938983917s
INFO:__main__:Time taken for 500M model: 435.830543756485s
INFO:__main__:Time per word for 1.5B model: 143.04770954142188 words per second
INFO:__main__:Time per word for 500 model : 272.25260298942607 words per second
INFO:__main__:Performance for 1.5B model:

INFO:sentiment_analysis:              precision    recall  f1-score   support

    negative       0.05      0.05      0.05       250
    positive       0.12      0.12      0.12       250

    accuracy                           0.09       500
   macro avg       0.08      0.09      0.08       500
weighted avg       0.08      0.09      0.08       500

INFO:__main__:Performance for 500M model:

INFO:sentiment_analysis:              precision    recall  f1-score   support

    negative       0.07      0.03      0.04       250
    positive       0.37      0.57      0.45       250

    accuracy                           0.30       500
   macro avg       0.22      0.30      0.25       500
weighted avg       0.22      0.30      0.25       500

INFO:__main__:Rendering confusion matrix for 1.5B model
INFO:__main__:Rendering confusion matrix for 500M model
INFO:__main__:Finished
