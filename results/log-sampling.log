INFO:__main__:Started
INFO:__main__:Using Apple Silicon / MPS GPU
INFO:__main__:1.5B checkpoint: Qwen/Qwen2.5-1.5B-Instruct
INFO:__main__:
================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
Qwen2ForCausalLM                                        --
├─Qwen2Model: 1-1                                       --
│    └─Embedding: 2-1                                   233,373,696
│    └─ModuleList: 2-2                                  --
│    │    └─Qwen2DecoderLayer: 3-1                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-2                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-3                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-4                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-5                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-6                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-7                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-8                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-9                      46,797,824
│    │    └─Qwen2DecoderLayer: 3-10                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-11                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-12                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-13                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-14                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-15                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-16                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-17                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-18                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-19                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-20                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-21                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-22                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-23                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-24                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-25                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-26                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-27                     46,797,824
│    │    └─Qwen2DecoderLayer: 3-28                     46,797,824
│    └─Qwen2RMSNorm: 2-3                                1,536
│    └─Qwen2RotaryEmbedding: 2-4                        --
├─Linear: 1-2                                           233,373,696
================================================================================
Total params: 1,777,088,000
Trainable params: 1,777,088,000
Non-trainable params: 0
================================================================================
INFO:__main__:500M model: Qwen/Qwen2.5-0.5B-Instruct
INFO:__main__:
================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
Qwen2ForCausalLM                                        --
├─Qwen2Model: 1-1                                       --
│    └─Embedding: 2-1                                   136,134,656
│    └─ModuleList: 2-2                                  --
│    │    └─Qwen2DecoderLayer: 3-1                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-2                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-3                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-4                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-5                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-6                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-7                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-8                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-9                      14,912,384
│    │    └─Qwen2DecoderLayer: 3-10                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-11                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-12                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-13                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-14                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-15                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-16                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-17                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-18                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-19                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-20                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-21                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-22                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-23                     14,912,384
│    │    └─Qwen2DecoderLayer: 3-24                     14,912,384
│    └─Qwen2RMSNorm: 2-3                                896
│    └─Qwen2RotaryEmbedding: 2-4                        --
├─Linear: 1-2                                           136,134,656
================================================================================
Total params: 630,167,424
Trainable params: 630,167,424
Non-trainable params: 0
================================================================================
INFO:__main__:The 1.5B model has a vocabulary of 151643 tokens.
INFO:__main__:The 500M model has a vocabulary of 151643 tokens.
INFO:__main__:Deriving a balanced random sample of 5%/500 reviews of IMDB the test set for evaluation
INFO:__main__:Downloading and reading IMDB dataset...
INFO:__main__:Test set statistics:
INFO:sentiment_analysis:Number of sentences:                  500
INFO:sentiment_analysis:Average number of words per sentence: 225.348
INFO:sentiment_analysis:Max number of words per sentence:     1057
INFO:sentiment_analysis:Min number of words per sentence:     23
INFO:sentiment_analysis:Total number of words:                112674
INFO:__main__:Time taken: 313.03211522102356s
INFO:sentiment_analysis:Top K:  400, Temperature: 1.0
INFO:__main__:[1.5B] Maximum accuracy with (temp, top_k) (1.0, 400): 0.075
INFO:__main__:Time taken: 123.06700897216797s
INFO:sentiment_analysis:Top K:  400, Temperature: 1.0
INFO:__main__:[500M] Maximum accuracy with (temp, top_k) (1.0, 400): 0.05
INFO:__main__:Time taken: 480.55625581741333s
INFO:__main__:Time taken: 321.58928203582764s
INFO:__main__:Time per word for 1.5B model: 234.46578550589157 words per second
INFO:__main__:Time per word for 500 model : 350.36615426582284 words per second
INFO:__main__:Performance for 1.5B model:

INFO:sentiment_analysis:              precision    recall  f1-score   support

       based       0.00      0.00      0.00         0
    negative       0.07      0.06      0.06       250
    positive       0.06      0.05      0.06       250
         the       0.00      0.00      0.00         0
          to       0.00      0.00      0.00         0

    accuracy                           0.06       500
   macro avg       0.03      0.02      0.02       500
weighted avg       0.06      0.06      0.06       500

INFO:__main__:Performance for 500M model:

INFO:sentiment_analysis:              precision    recall  f1-score   support

       based       0.00      0.00      0.00         0
    negative       0.12      0.02      0.03       250
    positive       0.11      0.06      0.08       250
         the       0.00      0.00      0.00         0
        this       0.00      0.00      0.00         0

    accuracy                           0.04       500
   macro avg       0.05      0.02      0.02       500
weighted avg       0.12      0.04      0.06       500

INFO:__main__:Rendering confusion matrix for 1.5B model
INFO:__main__:Rendering confusion matrix for 500M model
INFO:__main__:Finished
